{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9255f3-0d20-44b1-a81b-4575673a88bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1756/1756 [00:00<00:00, 93727.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "# Load the Yoochoose dataset and preprocess it to create session-based sequences of interactions\n",
    "data = pd.read_csv('yoochoose_dataset/filtered_clicks.dat',\n",
    "                   names=['session_id', 'timestamp', 'item_id', 'category'],\n",
    "                   dtype={'session_id': 'int64', 'timestamp': 'str', 'item_id': 'int64', 'category': 'int64'},\n",
    "                   parse_dates=['timestamp'])\n",
    "\n",
    "# Create item and session maps\n",
    "item_map = dict(zip(np.unique(data.item_id), range(len(np.unique(data.item_id)))))\n",
    "session_map = dict(zip(np.unique(data.session_id), range(len(np.unique(data.session_id)))))\n",
    "\n",
    "# Map item and session IDs\n",
    "data['item_id'] = data['item_id'].map(item_map)\n",
    "data['session_id'] = data['session_id'].map(session_map)\n",
    "\n",
    "# Sort by session and timestamp\n",
    "data = data.sort_values(['session_id', 'timestamp'])\n",
    "\n",
    "# Create next item and session columns\n",
    "data['next_item_id'] = data.groupby('session_id')['item_id'].shift(-1)\n",
    "data['next_session_id'] = data.groupby('session_id')['session_id'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "session_ids = data['session_id'].values.astype('int32')\n",
    "item_ids = data['item_id'].values.astype('int32')\n",
    "next_item_ids = data['next_item_id'].values.astype('int32')\n",
    "next_session_ids = data['next_session_id'].values.astype('int32')\n",
    "timestamps = data['timestamp'].values\n",
    "\n",
    "# Create a directed graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(item_map.values())\n",
    "\n",
    "# Add edges to the graph\n",
    "for session_id, items in tqdm(data.groupby('session_id')['item_id']):\n",
    "    items = items.values.tolist()\n",
    "    for i in range(len(items)-1):\n",
    "        src, dst = items[i], items[i+1]\n",
    "        graph.add_edge(src, dst)\n",
    "        \n",
    "        \n",
    "# Create dense feature matrix\n",
    "num_items = len(item_map)\n",
    "features = np.eye(num_items, dtype='float32')[item_ids]\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = nx.adjacency_matrix(graph, nodelist=range(num_items))\n",
    "adj_matrix = csr_matrix(adj_matrix)\n",
    "adj_indices = np.array(adj_matrix.nonzero()).T\n",
    "# Get adjacency matrix in a flat list\n",
    "adj_list = [adj_matrix[i, j] for i, j in adj_indices]\n",
    "# Convert adjacency list to a sparse tensor\n",
    "adj_values = tf.constant(adj_list, dtype=tf.float32)\n",
    "num_nodes = adj_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e41b13e-913a-4308-b7ed-ba0238417924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_items, num_features, hidden_units):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.input_layer = Input(shape=(1,), dtype=tf.int64)\n",
    "        self.embedding_layer = layers.Embedding(input_dim=self.num_items, output_dim=self.num_features)(self.input_layer)\n",
    "        self.embedding_layer = layers.Reshape(target_shape=(self.num_features,))(self.embedding_layer)\n",
    "\n",
    "        self.inputs_layer = Input(shape=(None, self.num_features))\n",
    "        self.adj_dense_layer = layers.Dense(units=self.num_items, activation='sigmoid')(self.embedding_layer)\n",
    "        self.adjacency_layer = tf.expand_dims(self.adj_dense_layer, axis=0)\n",
    "        self.features_layer = self.inputs_layer[:, :, 1:]\n",
    "        self.features_reshape_layer = layers.Reshape(target_shape=(tf.shape(self.inputs_layer)[1], self.num_features-1))(self.features_layer)\n",
    "        self.concat_layer = layers.Concatenate(axis=-1)([self.embedding_layer, self.features_reshape_layer])\n",
    "        self.gcn_layer = layers.Dense(units=self.hidden_units, activation='relu')(self.concat_layer)\n",
    "        self.flatten_layer = layers.Flatten()(self.gcn_layer)\n",
    "        self.outputs_layer = layers.Dense(units=self.num_items, activation='softmax')(self.flatten_layer)\n",
    "\n",
    "        self.model = Model(inputs=[self.input_layer, self.inputs_layer], outputs=self.outputs_layer)\n",
    "    \n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    positive_indices = tf.where(y_true > 0)\n",
    "    negative_indices = tf.where(y_true == 0)\n",
    "    positive_scores = tf.gather_nd(y_pred, positive_indices)\n",
    "    negative_scores = tf.gather_nd(y_pred, negative_indices)\n",
    "    positive_scores = tf.expand_dims(positive_scores, axis=1)\n",
    "    loss = tf.reduce_mean(tf.maximum(0.0, 1.0 - positive_scores + negative_scores))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba66c36-9405-49df-bdbf-4c6601b71632",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_indices, val_indices \u001b[38;5;241m=\u001b[39m train_test_split(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(session_ids)), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGNNModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile_model(learning_rate)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m, in \u001b[0;36mGNNModel.__init__\u001b[0;34m(self, num_items, num_features, hidden_units)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs_layer[:, :, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_flatten_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_layer)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_flatten_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_units, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_layer)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39mnum_items, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_layer)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py:1359\u001b[0m, in \u001b[0;36mTensorShape.as_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of integers or `None` for each dimension.\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;124;03m  ValueError: If `self` is an unknown shape with an unknown rank.\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1359\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_list() is not defined on an unknown TensorShape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims)\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 32\n",
    "num_features = features.shape[1]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_indices, val_indices = train_test_split(np.arange(len(session_ids)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = GNNModel(num_items, num_features, embedding_dim)\n",
    "model.compile_model(learning_rate)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    np.random.shuffle(train_indices)\n",
    "    pbar = tqdm(range(0, len(train_indices), batch_size))\n",
    "    for i in pbar:\n",
    "        batch_indices = train_indices[i:i+batch_size]\n",
    "        batch_session_ids = session_ids[batch_indices]\n",
    "        batch_item_ids = item_ids[batch_indices]\n",
    "        batch_features = features[batch_item_ids]\n",
    "        batch_next_item_ids = next_item_ids[batch_indices]\n",
    "        batch_labels = np.zeros((len(batch_indices), num_items))\n",
    "        for j, next_item_id in enumerate(batch_next_item_ids):\n",
    "            if next_item_id is not None:\n",
    "                batch_labels[j, next_item_id] = 1\n",
    "        batch_labels = np.argmax(batch_labels, axis=1)\n",
    "\n",
    "        # Create sparse tensor from adjacency matrix\n",
    "        batch_adj_values = adj_values[batch_item_ids]\n",
    "        batch_adj_indices = adj_indices[batch_item_ids]\n",
    "        batch_adj_indices[:, 0] -= i\n",
    "        batch_adj_indices[:, 1] -= i\n",
    "        batch_adj_indices += np.array([np.zeros_like(batch_adj_indices[:, 0]), batch_indices]).T.reshape(-1, 2) * num_nodes\n",
    "        batch_adj_indices = np.transpose(batch_adj_indices)\n",
    "        batch_adj_shape = (batch_size, num_nodes, num_nodes)\n",
    "        batch_adj_tensor = tf.sparse.SparseTensor(batch_adj_indices, batch_adj_values, batch_adj_shape)\n",
    "\n",
    "        # Train on batch\n",
    "        loss, accuracy = model.model.train_on_batch([batch_item_ids, batch_features, batch_adj_tensor], batch_labels)\n",
    "        pbar.set_description(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    val_session_ids = session_ids[val_indices]\n",
    "    val_item_ids = item_ids[val_indices]\n",
    "    val_features = features[val_item_ids]\n",
    "    val_next_item_ids = next_item_ids[val_indices]\n",
    "    val_labels = np.zeros((len(val_indices), num_items))\n",
    "    for j, next_item_id in enumerate(val_next_item_ids):\n",
    "        if next_item_id is not None:\n",
    "            val_labels[j, next_item_id] = 1\n",
    "    val_labels = np.argmax(val_labels, axis=1)\n",
    "\n",
    "    # Create sparse tensor from adjacency matrix\n",
    "    val_adj_values = adj_values[val_item_ids]\n",
    "    val_adj_indices = adj_indices[val_item_ids]\n",
    "    val_adj_indices[:, 1] += len(session_ids)\n",
    "    val_adj_indices = np.transpose(val_adj_indices)\n",
    "    val_adj_shape = (len(val_indices), num_nodes, num_nodes)\n",
    "    val_adj_tensor = tf.sparse.SparseTensor(val_adj_indices, val_adj_values, val_adj_shape)\n",
    "\n",
    "    val_loss, val_accuracy = model.model.evaluate([val_item_ids, val_features, val_adj_tensor], val_labels, verbose=0)\n",
    "    print(f'Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print('Training complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fba163-8739-4634-a4b6-f4f51cb6d14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
