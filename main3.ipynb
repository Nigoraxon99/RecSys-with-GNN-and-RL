{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923fe83-4307-42a7-88c0-f582ad31ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_graph(train_data):\n",
    "    graph = nx.DiGraph()\n",
    "    for seq in train_data:\n",
    "        for i in range(len(seq) - 1):\n",
    "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
    "                weight = 1\n",
    "            else:\n",
    "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
    "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
    "    for node in graph.nodes:\n",
    "        sum = 0\n",
    "        for j, i in graph.in_edges(node):\n",
    "            sum += graph.get_edge_data(j, i)['weight']\n",
    "        if sum != 0:\n",
    "            for j, i in graph.in_edges(i):\n",
    "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def data_masks(all_usr_pois, item_tail):\n",
    "    us_lens = [len(upois) for upois in all_usr_pois]\n",
    "    len_max = max(us_lens)\n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "    return us_pois, us_msks, len_max\n",
    "\n",
    "\n",
    "def split_validation(train_set, valid_portion):\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
    "\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, data, sub_graph=False, method='ggnn', sparse=False, shuffle=False):\n",
    "        inputs = data[0]\n",
    "        inputs, mask, len_max = data_masks(inputs, [0])\n",
    "        self.inputs = np.asarray(inputs)\n",
    "        self.mask = np.asarray(mask)\n",
    "        self.len_max = len_max\n",
    "        self.targets = np.asarray(data[1])\n",
    "        self.length = len(inputs)\n",
    "        self.shuffle = shuffle\n",
    "        self.sub_graph = sub_graph\n",
    "        self.sparse = sparse\n",
    "        self.method = method\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            shuffled_arg = np.arange(self.length)\n",
    "            np.random.shuffle(shuffled_arg)\n",
    "            self.inputs = self.inputs[shuffled_arg]\n",
    "            self.mask = self.mask[shuffled_arg]\n",
    "            self.targets = self.targets[shuffled_arg]\n",
    "        n_batch = int(self.length / batch_size)\n",
    "        if self.length % batch_size != 0:\n",
    "            n_batch += 1\n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        slices[-1] = np.arange(self.length - batch_size, self.length)\n",
    "        return slices\n",
    "\n",
    "    def get_slice(self, index):\n",
    "        if 1:\n",
    "            items, n_node, A_in, A_out, alias_inputs = [], [], [], [], []\n",
    "            for u_input in self.inputs[index]:\n",
    "                n_node.append(len(np.unique(u_input)))\n",
    "            max_n_node = np.max(n_node)\n",
    "            if self.method == 'ggnn':\n",
    "                for u_input in self.inputs[index]:\n",
    "                    node = np.unique(u_input)\n",
    "                    items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "                    u_A = np.zeros((max_n_node, max_n_node))\n",
    "                    for i in np.arange(len(u_input) - 1):\n",
    "                        if u_input[i + 1] == 0:\n",
    "                            break\n",
    "                        u = np.where(node == u_input[i])[0][0]\n",
    "                        v = np.where(node == u_input[i + 1])[0][0]\n",
    "                        u_A[u][v] = 1\n",
    "                    u_sum_in = np.sum(u_A, 0)\n",
    "                    u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "                    u_A_in = np.divide(u_A, u_sum_in)\n",
    "                    u_sum_out = np.sum(u_A, 1)\n",
    "                    u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "                    u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "\n",
    "                    A_in.append(u_A_in)\n",
    "                    A_out.append(u_A_out)\n",
    "                    alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "                return A_in, A_out, alias_inputs, items, self.mask[index], self.targets[index]\n",
    "            elif self.method == 'gat':\n",
    "                A_in = []\n",
    "                A_out = []\n",
    "                for u_input in self.inputs[index]:\n",
    "                    node = np.unique(u_input)\n",
    "                    items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "                    u_A = np.eye(max_n_node)\n",
    "                    for i in np.arange(len(u_input) - 1):\n",
    "                        if u_input[i + 1] == 0:\n",
    "                            break\n",
    "                        u = np.where(node == u_input[i])[0][0]\n",
    "                        v = np.where(node == u_input[i + 1])[0][0]\n",
    "                        u_A[u][v] = 1\n",
    "                    A_in.append(-1e9 * (1 - u_A))\n",
    "                    A_out.append(-1e9 * (1 - u_A.transpose()))\n",
    "                    alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "                return A_in, A_out, alias_inputs, items, self.mask[index], self.targets[index]\n",
    "\n",
    "        else:\n",
    "            return self.inputs[index], self.mask[index], self.targets[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9d17c-a2c3-42f1-8617-cdc6ee95b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import math\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, hidden_size=100, out_size=100, batch_size=100, nonhybrid=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mask = tf.placeholder(dtype=tf.float32)\n",
    "        self.alias = tf.placeholder(dtype=tf.int32)  # 给给每个输入重新\n",
    "        self.item = tf.placeholder(dtype=tf.int32)   # 重新编号的序列构成的矩阵\n",
    "        self.tar = tf.placeholder(dtype=tf.int32)\n",
    "        self.nonhybrid = nonhybrid\n",
    "        self.stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "\n",
    "        self.nasr_w1 = tf.get_variable('nasr_w1', [self.out_size, self.out_size], dtype=tf.float32,\n",
    "                                       initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.nasr_w2 = tf.get_variable('nasr_w2', [self.out_size, self.out_size], dtype=tf.float32,\n",
    "                                       initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.nasr_v = tf.get_variable('nasrv', [1, self.out_size], dtype=tf.float32,\n",
    "                                      initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.nasr_b = tf.get_variable('nasr_b', [self.out_size], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "    def forward(self, re_embedding, train=True):\n",
    "        rm = tf.reduce_sum(self.mask, 1)\n",
    "        last_id = tf.gather_nd(self.alias, tf.stack([tf.range(self.batch_size), tf.to_int32(rm)-1], axis=1))\n",
    "        last_h = tf.gather_nd(re_embedding, tf.stack([tf.range(self.batch_size), last_id], axis=1))\n",
    "        seq_h = tf.stack([tf.nn.embedding_lookup(re_embedding[i], self.alias[i]) for i in range(self.batch_size)],\n",
    "                         axis=0)                                                           #batch_size*T*d\n",
    "        last = tf.matmul(last_h, self.nasr_w1)\n",
    "        seq = tf.matmul(tf.reshape(seq_h, [-1, self.out_size]), self.nasr_w2)\n",
    "        last = tf.reshape(last, [self.batch_size, 1, -1])\n",
    "        m = tf.nn.sigmoid(last + tf.reshape(seq, [self.batch_size, -1, self.out_size]) + self.nasr_b)\n",
    "        coef = tf.matmul(tf.reshape(m, [-1, self.out_size]), self.nasr_v, transpose_b=True) * tf.reshape(\n",
    "            self.mask, [-1, 1])\n",
    "        b = self.embedding[1:]\n",
    "        if not self.nonhybrid:\n",
    "            ma = tf.concat([tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1),\n",
    "                            tf.reshape(last, [-1, self.out_size])], -1)\n",
    "            self.B = tf.get_variable('B', [2 * self.out_size, self.out_size],\n",
    "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "            y1 = tf.matmul(ma, self.B)\n",
    "            logits = tf.matmul(y1, b, transpose_b=True)\n",
    "        else:\n",
    "            ma = tf.reduce_sum(tf.reshape(coef, [self.batch_size, -1, 1]) * seq_h, 1)\n",
    "            logits = tf.matmul(ma, b, transpose_b=True)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.tar - 1, logits=logits))\n",
    "        self.vars = tf.trainable_variables()\n",
    "        if train:\n",
    "            lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in self.vars if v.name not\n",
    "                               in ['bias', 'gamma', 'b', 'g', 'beta']]) * self.L2\n",
    "            loss = loss + lossL2\n",
    "        return loss, logits\n",
    "\n",
    "    def run(self, fetches, tar, item, adj_in, adj_out, alias, mask):\n",
    "        return self.sess.run(fetches, feed_dict={self.tar: tar, self.item: item, self.adj_in: adj_in,\n",
    "                                                 self.adj_out: adj_out, self.alias: alias, self.mask: mask})\n",
    "\n",
    "\n",
    "class GGNN(Model):\n",
    "    def __init__(self,hidden_size=100, out_size=100, batch_size=300, n_node=None,\n",
    "                 lr=None, l2=None, step=1, decay=None, lr_dc=0.1, nonhybrid=False):\n",
    "        super(GGNN,self).__init__(hidden_size, out_size, batch_size, nonhybrid)\n",
    "        self.embedding = tf.get_variable(shape=[n_node, hidden_size], name='embedding', dtype=tf.float32,\n",
    "                                         initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.adj_in = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n",
    "        self.adj_out = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, None])\n",
    "        self.n_node = n_node\n",
    "        self.L2 = l2\n",
    "        self.step = step\n",
    "        self.nonhybrid = nonhybrid\n",
    "        self.W_in = tf.get_variable('W_in', shape=[self.out_size, self.out_size], dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.b_in = tf.get_variable('b_in', [self.out_size], dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.W_out = tf.get_variable('W_out', [self.out_size, self.out_size], dtype=tf.float32,\n",
    "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        self.b_out = tf.get_variable('b_out', [self.out_size], dtype=tf.float32,\n",
    "                                     initializer=tf.random_uniform_initializer(-self.stdv, self.stdv))\n",
    "        with tf.variable_scope('ggnn_model', reuse=None):\n",
    "            self.loss_train, _ = self.forward(self.ggnn())\n",
    "        with tf.variable_scope('ggnn_model', reuse=True):\n",
    "            self.loss_test, self.score_test = self.forward(self.ggnn(), train=False)\n",
    "        self.global_step = tf.Variable(0)\n",
    "        self.learning_rate = tf.train.exponential_decay(lr, global_step=self.global_step, decay_steps=decay,\n",
    "                                                        decay_rate=lr_dc, staircase=True)\n",
    "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_train, global_step=self.global_step)\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "        config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def ggnn(self):\n",
    "        fin_state = tf.nn.embedding_lookup(self.embedding, self.item)\n",
    "        cell = tf.nn.rnn_cell.GRUCell(self.out_size)\n",
    "        with tf.variable_scope('gru'):\n",
    "            for i in range(self.step):\n",
    "                fin_state = tf.reshape(fin_state, [self.batch_size, -1, self.out_size])\n",
    "                fin_state_in = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),\n",
    "                                                    self.W_in) + self.b_in, [self.batch_size, -1, self.out_size])\n",
    "                fin_state_out = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),\n",
    "                                                     self.W_out) + self.b_out, [self.batch_size, -1, self.out_size])\n",
    "                av = tf.concat([tf.matmul(self.adj_in, fin_state_in),\n",
    "                                tf.matmul(self.adj_out, fin_state_out)], axis=-1)\n",
    "                state_output, fin_state = \\\n",
    "                    tf.nn.dynamic_rnn(cell, tf.expand_dims(tf.reshape(av, [-1, 2*self.out_size]), axis=1),\n",
    "                                      initial_state=tf.reshape(fin_state, [-1, self.out_size]))\n",
    "        return tf.reshape(fin_state, [self.batch_size, -1, self.out_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02220cea-6313-4bba-95f7-3a28e4999f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from model import *\n",
    "from utils import build_graph, Data, split_validation\n",
    "import pickle\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='sample', help='dataset name: diginetica/yoochoose1_4/yoochoose1_64/sample')\n",
    "parser.add_argument('--method', type=str, default='ggnn', help='ggnn/gat/gcn')\n",
    "parser.add_argument('--validation', action='store_true', help='validation')\n",
    "parser.add_argument('--epoch', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--batchSize', type=int, default=100, help='input batch size')\n",
    "parser.add_argument('--hiddenSize', type=int, default=100, help='hidden state size')\n",
    "parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--step', type=int, default=1, help='gnn propogation steps')\n",
    "parser.add_argument('--nonhybrid', action='store_true', help='global preference')\n",
    "parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n",
    "parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n",
    "opt = parser.parse_args()\n",
    "train_data = pickle.load(open('../datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "test_data = pickle.load(open('../datasets/' + opt.dataset + '/test.txt', 'rb'))\n",
    "# all_train_seq = pickle.load(open('../datasets/' + opt.dataset + '/all_train_seq.txt', 'rb'))\n",
    "if opt.dataset == 'diginetica':\n",
    "    n_node = 43098\n",
    "elif opt.dataset == 'yoochoose1_64' or opt.dataset == 'yoochoose1_4':\n",
    "    n_node = 37484\n",
    "else:\n",
    "    n_node = 310\n",
    "# g = build_graph(all_train_seq)\n",
    "train_data = Data(train_data, sub_graph=True, method=opt.method, shuffle=True)\n",
    "test_data = Data(test_data, sub_graph=True, method=opt.method, shuffle=False)\n",
    "model = GGNN(hidden_size=opt.hiddenSize, out_size=opt.hiddenSize, batch_size=opt.batchSize, n_node=n_node,\n",
    "                 lr=opt.lr, l2=opt.l2,  step=opt.step, decay=opt.lr_dc_step * len(train_data.inputs) / opt.batchSize, lr_dc=opt.lr_dc,\n",
    "                 nonhybrid=opt.nonhybrid)\n",
    "print(opt)\n",
    "best_result = [0, 0]\n",
    "best_epoch = [0, 0]\n",
    "for epoch in range(opt.epoch):\n",
    "    print('epoch: ', epoch, '===========================================')\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    fetches = [model.opt, model.loss_train, model.global_step]\n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    loss_ = []\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        adj_in, adj_out, alias, item, mask, targets = train_data.get_slice(i)\n",
    "        _, loss, _ = model.run(fetches, targets, item, adj_in, adj_out, alias,  mask)\n",
    "        loss_.append(loss)\n",
    "    loss = np.mean(loss_)\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    hit, mrr, test_loss_ = [], [],[]\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        adj_in, adj_out, alias, item, mask, targets = test_data.get_slice(i)\n",
    "        scores, test_loss = model.run([model.score_test, model.loss_test], targets, item, adj_in, adj_out, alias,  mask)\n",
    "        test_loss_.append(test_loss)\n",
    "        index = np.argsort(scores, 1)[:, -20:]\n",
    "        for score, target in zip(index, targets):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (20-np.where(score == target - 1)[0][0]))\n",
    "    hit = np.mean(hit)*100\n",
    "    mrr = np.mean(mrr)*100\n",
    "    test_loss = np.mean(test_loss_)\n",
    "    if hit >= best_result[0]:\n",
    "        best_result[0] = hit\n",
    "        best_epoch[0] = epoch\n",
    "    if mrr >= best_result[1]:\n",
    "        best_result[1] = mrr\n",
    "        best_epoch[1]=epoch\n",
    "    print('train_loss:\\t%.4f\\ttest_loss:\\t%4f\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'%\n",
    "          (loss, test_loss, best_result[0], best_result[1], best_epoch[0], best_epoch[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
