{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085827e1-8e3b-460d-a345-9df78aa9b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1756/1756 [00:00<00:00, 70231.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy as sp\n",
    "\n",
    "# Data preprocessing\n",
    "# Load the Yoochoose dataset and preprocess it to create session-based sequences of interactions\n",
    "data = pd.read_csv('yoochoose_dataset/filtered_clicks.dat',\n",
    "                   names=['session_id', 'timestamp', 'item_id', 'category'],\n",
    "                   dtype={'session_id': 'int64', 'timestamp': 'str', 'item_id': 'int64', 'category': 'int64'},\n",
    "                   parse_dates=['timestamp'])\n",
    "\n",
    "# Create item and session maps\n",
    "item_map = dict(zip(np.unique(data.item_id), range(len(np.unique(data.item_id)))))\n",
    "session_map = dict(zip(np.unique(data.session_id), range(len(np.unique(data.session_id)))))\n",
    "\n",
    "# Map item and session IDs\n",
    "data['item_id'] = data['item_id'].map(item_map)\n",
    "data['session_id'] = data['session_id'].map(session_map)\n",
    "\n",
    "# Sort by session and timestamp\n",
    "data = data.sort_values(['session_id', 'timestamp'])\n",
    "\n",
    "# Create next item and session columns\n",
    "data['next_item_id'] = data.groupby('session_id')['item_id'].shift(-1)\n",
    "data['next_session_id'] = data.groupby('session_id')['session_id'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "session_ids = data['session_id'].values.astype('int32')\n",
    "item_ids = data['item_id'].values.astype('int32')\n",
    "next_item_ids = data['next_item_id'].values.astype('int32')\n",
    "next_session_ids = data['next_session_id'].values.astype('int32')\n",
    "timestamps = data['timestamp'].values\n",
    "\n",
    "# Create a directed graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(item_map.values())\n",
    "\n",
    "# Add edges to the graph\n",
    "for session_id, items in tqdm(data.groupby('session_id')['item_id']):\n",
    "    items = items.values.tolist()\n",
    "    for i in range(len(items)-1):\n",
    "        src, dst = items[i], items[i+1]\n",
    "        graph.add_edge(src, dst)\n",
    "        \n",
    "        \n",
    "# Create dense feature matrix\n",
    "num_items = len(item_map)\n",
    "features = np.eye(num_items, dtype='float32')[item_ids]\n",
    "\n",
    "# # Create adjacency matrix\n",
    "# adj_matrix = sp.sparse.coo_matrix(nx.to_numpy_array(graph, weight='weight', dtype=np.float32))\n",
    "# adj_matrix = tf.sparse.SparseTensor(indices=np.array([adj_matrix.row, adj_matrix.col]).T,\n",
    "#                                     values=adj_matrix.data,\n",
    "#                                     dense_shape=adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e125e7-3a64-4ec8-9267-2ec4b71b2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will define the GNN model. \n",
    "# We will use a simple GNN with a single hidden layer, \n",
    "# and we will use the mean squared error as the loss function.\n",
    "\n",
    "class GNNModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation=\"softmax\")\n",
    "        self.adj_matrix = tf.sparse.SparseTensor(indices=adj_matrix[0], values=adj_matrix[1], dense_shape=adj_matrix[2])\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Embed the input items\n",
    "        item_embeddings = tf.one_hot(inputs, depth=self.input_dim)\n",
    "        \n",
    "        # Propagate the embeddings through the graph\n",
    "        hidden_embeddings = tf.matmul(tf.matmul(self.adj_matrix, item_embeddings), self.hidden_layer.kernel)\n",
    "        hidden_embeddings = self.hidden_layer(hidden_embeddings)\n",
    "        \n",
    "        # Aggregate the embeddings of the neighboring items\n",
    "        neighbors = tf.matmul(self.adj_matrix, hidden_embeddings)\n",
    "        neighbors_agg = tf.reduce_sum(neighbors, axis=1)\n",
    "        # Concatenate the item embeddings with the aggregated neighbor embeddings\n",
    "        embeddings = tf.concat([item_embeddings, neighbors_agg], axis=1)\n",
    "    \n",
    "        # Predict the next item\n",
    "        outputs = self.output_layer(embeddings)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388820e6-d3b2-4cbe-a9d1-dc7142599018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(item_ids, session_ids):\n",
    "    session_to_index = {}\n",
    "    item_to_index = {}\n",
    "    session_index = 0\n",
    "    item_index = 0\n",
    "    data = []\n",
    "\n",
    "    for item, session in zip(item_ids, session_ids):\n",
    "        if session not in session_to_index:\n",
    "            session_to_index[session] = session_index\n",
    "            session_index += 1\n",
    "        if item not in item_to_index:\n",
    "            item_to_index[item] = item_index\n",
    "            item_index += 1\n",
    "        data.append((session_to_index[session], item_to_index[item]))\n",
    "\n",
    "    row, col = zip(*data)\n",
    "    adj_matrix = sp.sparse.coo_matrix(([1]*len(row), (row, col)), shape=(session_index, item_index))\n",
    "    return adj_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c07db1-62c3-4e0a-bd44-5976e6afc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation=\"softmax\")\n",
    "        self.adj_matrix = tf.sparse.SparseTensor(indices=adj_matrix[0], values=adj_matrix[1], dense_shape=adj_matrix[2])\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Embed the input session\n",
    "        session_embeddings = tf.one_hot(inputs, depth=self.input_dim)\n",
    "        \n",
    "        # Propagate the embeddings through the graph\n",
    "        hidden_embeddings = tf.matmul(tf.matmul(self.adj_matrix, session_embeddings), self.hidden_layer.kernel)\n",
    "        hidden_embeddings = self.hidden_layer(hidden_embeddings)\n",
    "        \n",
    "        # Aggregate the embeddings of the neighboring sessions\n",
    "        neighbors = tf.matmul(self.adj_matrix, hidden_embeddings)\n",
    "        neighbors_agg = tf.reduce_sum(neighbors, axis=1)\n",
    "        \n",
    "        # Concatenate the session embeddings with the aggregated neighbor embeddings\n",
    "        embeddings = tf.concat([session_embeddings, neighbors_agg], axis=1)\n",
    "        \n",
    "        # Predict the next item probability distribution\n",
    "        outputs = self.output_layer(embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ValueNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(output_dim, activation=\"linear\")\n",
    "        self.adj_matrix = tf.sparse.SparseTensor(indices=adj_matrix[0], values=adj_matrix[1], dense_shape=adj_matrix[2])\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Embed the input session\n",
    "        session_embeddings = tf.one_hot(inputs, depth=self.input_dim)\n",
    "        \n",
    "        # Propagate the embeddings through the graph\n",
    "        hidden_embeddings = tf.matmul(tf.matmul(self.adj_matrix, session_embeddings), self.hidden_layer.kernel)\n",
    "        hidden_embeddings = self.hidden_layer(hidden_embeddings)\n",
    "        \n",
    "        # Aggregate the embeddings of the neighboring sessions\n",
    "        neighbors = tf.matmul(self.adj_matrix, hidden_embeddings)\n",
    "        neighbors_agg = tf.reduce_sum(neighbors, axis=1)\n",
    "        \n",
    "        # Concatenate the session embeddings with the aggregated neighbor embeddings\n",
    "        embeddings = tf.concat([session_embeddings, neighbors_agg], axis=1)\n",
    "        \n",
    "        # Predict the value of the session\n",
    "        outputs = self.output_layer(embeddings)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b98faf6-8a65-416d-b333-e7cc4cf2080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will define the self-supervised reinforcement learning framework. \n",
    "# The framework consists of a policy network and a value network. \n",
    "# The policy network takes the current session as input and outputs a probability distribution over the items in the session. \n",
    "# The value network takes the current session and the recommended item as input and outputs a scalar value representing the expected future reward. \n",
    "# We will train the policy network using policy gradient and the value network using mean squared error.\n",
    "\n",
    "class SessionRecommender:\n",
    "    def __init__(self, data, input_dim, hidden_dim, output_dim, learning_rate, alpha):\n",
    "        adj_matrix = get_adjacency_matrix(item_ids, session_ids)\n",
    "        self.policy_network = GNNModel(input_dim, hidden_dim, output_dim, adj_matrix.tocsr())\n",
    "        self.value_network = GNNModel(input_dim+output_dim, hidden_dim, 1, adj_matrix.tocsr())\n",
    "        self.optimizer_policy = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.optimizer_value = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def get_action(self, session):\n",
    "        session = np.array([session])\n",
    "        item_prob_policy = self.policy_network(session).numpy().ravel()\n",
    "        item_prob_gnn = self.gnn_model(session).numpy().ravel()\n",
    "        item_prob_combined = self.alpha * item_prob_policy + (1 - self.alpha) * item_prob_gnn\n",
    "        # computes the probability distributions output by the policy_network and the gnn_model, \n",
    "        # and then combines them using linear combination\n",
    "        item_id_rec = np.random.choice(len(item_prob_combined), p=item_prob_combined)\n",
    "        return item_id_rec\n",
    "\n",
    "    \n",
    "    def train(self, session, action, reward):\n",
    "        # Compute the expected future reward using the value network\n",
    "        session_expanded = tf.expand_dims(session, axis=0)\n",
    "        action_expanded = tf.expand_dims(action, axis=0)\n",
    "        session_action = tf.concat([session_expanded, tf.one_hot(action_expanded, depth=self.policy_network.input_dim)], axis=1)\n",
    "        expected_reward = self.value_network(session_action)\n",
    "        \n",
    "        # Compute the advantage\n",
    "        baseline = self.value_network(session_expanded)\n",
    "        advantage = reward - baseline\n",
    "        \n",
    "        # Train the policy network using policy gradient\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.policy_network(session)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            log_prob = tf.math.log(probs[0][action])\n",
    "            loss_policy = -log_prob * advantage\n",
    "        \n",
    "        grads_policy = tape.gradient(loss_policy, self.policy_network.trainable_variables)\n",
    "        self.optimizer_policy.apply_gradients(zip(grads_policy, self.policy_network.trainable_variables))\n",
    "        \n",
    "        # Train the value network using mean squared error\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = self.value_network(session_action)\n",
    "            loss_value = tf.keras.losses.mean_squared_error(tf.constant([[reward]]), value)\n",
    "        \n",
    "        grads_value = tape.gradient(loss_value, self.value_network.trainable_variables)\n",
    "        self.optimizer_value.apply_gradients(zip(grads_value, self.value_network.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c638abaa-ecbd-4898-ae62-ae58279b4070",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/Users/nigorakhonganieva/miniforge3/envs/data-science/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 345, in __len__\n    raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\nTypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m item_encoder\u001b[38;5;241m.\u001b[39mfit(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the recommender system\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m recommender \u001b[38;5;241m=\u001b[39m \u001b[43mSessionRecommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mSessionRecommender.__init__\u001b[0;34m(self, data, input_dim, hidden_dim, output_dim, learning_rate, alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, input_dim, hidden_dim, output_dim, learning_rate, alpha):\n\u001b[1;32m      9\u001b[0m     adj_matrix \u001b[38;5;241m=\u001b[39m get_adjacency_matrix(item_ids, session_ids)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network \u001b[38;5;241m=\u001b[39m \u001b[43mGNNModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocsr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_network \u001b[38;5;241m=\u001b[39m GNNModel(input_dim\u001b[38;5;241m+\u001b[39moutput_dim, hidden_dim, \u001b[38;5;241m1\u001b[39m, adj_matrix\u001b[38;5;241m.\u001b[39mtocsr())\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_policy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mGNNModel.__init__\u001b[0;34m(self, input_dim, hidden_dim, output_dim, adj_matrix)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(hidden_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(output_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:125\u001b[0m, in \u001b[0;36mSparseTensor.__init__\u001b[0;34m(self, indices, values, dense_shape)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `SparseTensor`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    unknown or contains unknown elements (None or -1).\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, [indices, values, dense_shape]):\n\u001b[0;32m--> 125\u001b[0m   indices \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;66;03m# TODO(touts): Consider adding mutable_values() when 'values'\u001b[39;00m\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;66;03m# is a VariableOp and updating users of SparseTensor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m   values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(values, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1636\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1627\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1629\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1632\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1633\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1639\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\nTraceback (most recent call last):\n\n  File \"/Users/nigorakhonganieva/miniforge3/envs/data-science/lib/python3.9/site-packages/scipy/sparse/_base.py\", line 345, in __len__\n    raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\nTypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]\n\n"
     ]
    }
   ],
   "source": [
    "# Encode the items\n",
    "item_encoder = LabelEncoder()\n",
    "# item_encoder = OneHotEncoder(sparse=False)\n",
    "item_encoder.fit(data[\"item_id\"])\n",
    "\n",
    "\n",
    "# Train the recommender system\n",
    "recommender = SessionRecommender(data, len(item_encoder.classes_), hidden_dim=128, output_dim=64, learning_rate=1e-3, alpha=0.5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "    for session_id, group in train_data.groupby(\"session_id\"):\n",
    "        session_items = group[\"item_id\"].tolist()\n",
    "        session_items_encoded = item_encoder.transform(session_items)\n",
    "        for i in range(len(session_items_encoded)):\n",
    "            # Get the current state and action\n",
    "            state = session_items_encoded[:i]\n",
    "            action, action_prob = recommender.get_action(state)\n",
    "            recommended_item = item_encoder.inverse_transform([action])[0]\n",
    "            \n",
    "            # Get the reward for the action\n",
    "            if i == len(session_items_encoded) - 1:\n",
    "                reward = 1 if recommended_item == session_items[-1] else 0\n",
    "            else:\n",
    "                reward = 0\n",
    "                \n",
    "            # Train the recommender system\n",
    "            recommender.train(state, action, reward)\n",
    "            \n",
    "            # Update the total reward and total steps\n",
    "            total_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Print the progress\n",
    "            if total_steps % 1000 == 0:\n",
    "                print(f\"Processed {total_steps} steps, average reward: {total_reward / total_steps:.3f}\")\n",
    "                \n",
    "    # Evaluate the recommender system on the test set\n",
    "    if test_data.empty:\n",
    "        print(\"Test data is empty, skipping evaluation\")\n",
    "        continue\n",
    "    \n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    mse = 0\n",
    "    for session_id, group in test_data.groupby(\"session_id\"):\n",
    "        session_items = group[\"item_id\"].tolist()\n",
    "        session_items_encoded = item_encoder.transform(session_items)\n",
    "        for i in range(len(session_items_encoded)-1):\n",
    "            state = session_items_encoded[:i+1]\n",
    "            next_item = session_items_encoded[i+1]\n",
    "            logits = recommender.policy_network(state)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            top_k = tf.math.top_k(probs, k=10)[1][0]\n",
    "            recommended_items = item_encoder.inverse_transform(top_k)\n",
    "            if next_item in top_k:\n",
    "                hits.append(1)\n",
    "                ndcgs.append(1 / math.log(top_k.index(next_item)+2))\n",
    "            else:\n",
    "                hits.append(0)\n",
    "                ndcgs.append(0)\n",
    "            mse += (logits[0][next_item] - 1) ** 2\n",
    "            \n",
    "    precision = np.mean(hits)\n",
    "    recall = np.mean(hits) / len(item_encoder.classes_)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "    mse = mse / len(test_data)\n",
    "    \n",
    "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, NDCG: {ndcg:.3f}, MSE: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74ca18-fe4d-4db2-b4ad-edea5ef43f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f00e1b-724d-45d5-837b-0eb1ddab7fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
