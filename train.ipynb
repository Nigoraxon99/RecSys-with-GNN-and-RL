{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcdd1698-1d5a-45f4-af62-9a19d1a25c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy as sp\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f665f3e-5bb7-4152-92c4-396c20da71b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3007, 3007)\n",
      "3007\n",
      "Epoch 0/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'SparseTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Compute target Q values using critic model\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# Predict Q values for the next state\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     q_values_next \u001b[38;5;241m=\u001b[39m critic_model(batch_next_item_ids, actor_model(\u001b[43mbatch_adj_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_item_ids\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Compute target Q values\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mones_like(q_values_next)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SparseTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data preprocessing\n",
    "# Load the Yoochoose dataset and preprocess it to create session-based sequences of interactions\n",
    "data = pd.read_csv('yoochoose_dataset/filtered_clicks.dat',\n",
    "                   names=['session_id', 'timestamp', 'item_id', 'category'],\n",
    "                   dtype={'session_id': 'int64', 'timestamp': 'str', 'item_id': 'int64', 'category': 'int64'},\n",
    "                   parse_dates=['timestamp'])\n",
    "\n",
    "# Create item and session maps\n",
    "item_map = dict(zip(np.unique(data.item_id), range(len(np.unique(data.item_id)))))\n",
    "session_map = dict(zip(np.unique(data.session_id), range(len(np.unique(data.session_id)))))\n",
    "\n",
    "# Map item and session IDs\n",
    "data['item_id'] = data['item_id'].map(item_map)\n",
    "data['session_id'] = data['session_id'].map(session_map)\n",
    "\n",
    "# Sort by session and timestamp\n",
    "data = data.sort_values(['session_id', 'timestamp'])\n",
    "\n",
    "# Create next item and session columns\n",
    "data['next_item_id'] = data.groupby('session_id')['item_id'].shift(-1)\n",
    "data['next_session_id'] = data.groupby('session_id')['session_id'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "session_ids = data['session_id'].values.astype('int32')\n",
    "item_ids = data['item_id'].values.astype('int32')\n",
    "next_item_ids = data['next_item_id'].values.astype('int32')\n",
    "next_session_ids = data['next_session_id'].values.astype('int32')\n",
    "timestamps = data['timestamp'].values\n",
    "\n",
    "# Create a directed graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(item_map.values())\n",
    "\n",
    "# Add edges between items that co-occur in the same session\n",
    "for session_id in np.unique(session_ids):\n",
    "    items_in_session = item_ids[session_ids == session_id]\n",
    "    for i in range(len(items_in_session)):\n",
    "        for j in range(i + 1, len(items_in_session)):\n",
    "            if not graph.has_edge(items_in_session[i], items_in_session[j]):\n",
    "                graph.add_edge(items_in_session[i], items_in_session[j], weight=0)\n",
    "            graph[items_in_session[i]][items_in_session[j]]['weight'] += 1\n",
    "\n",
    "# Normalize edge weights\n",
    "for u, v, d in graph.edges(data=True):\n",
    "    d['weight'] /= np.sqrt(graph.degree(u) * graph.degree(v))            \n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = coo_matrix(nx.to_numpy_array(graph, weight='weight', dtype=np.float32))\n",
    "adj_matrix = tf.sparse.SparseTensor(indices=np.array([adj_matrix.row, adj_matrix.col]).T,\n",
    "                                    values=adj_matrix.data,\n",
    "                                    dense_shape=adj_matrix.shape)    \n",
    "print(adj_matrix.shape)\n",
    "num_nodes = adj_matrix.shape[0] \n",
    "    \n",
    "\n",
    "num_items = len(item_map)\n",
    "print(num_items)\n",
    "    \n",
    "embedding_dim = 32\n",
    "num_layers = 2\n",
    "hidden_dim = 32\n",
    "\n",
    "\n",
    "class GNNActor(tf.keras.Model):\n",
    "    def __init__(self, num_items, num_features, num_edge_features, hidden_dim):\n",
    "        super(GNNActor, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(None, num_features + num_edge_features), sparse=True)\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define node embedding layer\n",
    "        self.node_embedding = tf.keras.layers.Embedding(input_dim=num_items, output_dim=hidden_dim, input_length=1)\n",
    "\n",
    "        # Define graph convolutional layers\n",
    "        self.gcn_layer1 = tf.keras.layers.Dense(units=hidden_dim, activation='relu')\n",
    "        self.gcn_layer2 = tf.keras.layers.Dense(units=hidden_dim, activation='relu')\n",
    "        \n",
    "        # Define final prediction layer\n",
    "        self.prediction_layer = tf.keras.layers.Dense(units=num_items, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Check input shape and extract node and edge features accordingly\n",
    "        print(f'inputs.shape {inputs.shape}')\n",
    "        if len(inputs.shape) == 1:\n",
    "            # print(f'GNN inputs shape: {inputs.shape}')\n",
    "            node_features = tf.expand_dims(tf.range(self.num_items), axis=-1)\n",
    "            edge_features = inputs\n",
    "        else:\n",
    "            inputs = tf.reshape(inputs, shape=(-1, self.num_features + self.num_edge_features))\n",
    "            node_features, edge_features = tf.unstack(inputs, axis=1)\n",
    "        # Node embedding layer\n",
    "        # print(f'node_features shape: {node_features.shape}')\n",
    "        node_embeddings = self.node_embedding(node_features) # (batch_size, 1, hidden_dim)\n",
    "        # print(f'node_embeddings shape: {node_embeddings.shape}')\n",
    "        # print(f'node_embeddings: {node_embeddings}')\n",
    "        # print(f'edge_features shape: {edge_features.shape}')\n",
    "        # print(f'edge_features: {edge_features}')\n",
    "\n",
    "        # Reshape edge_features tensor\n",
    "        edge_features = tf.expand_dims(edge_features, axis=-1) # (batch_size, num_edge_features, num_edges, 1)\n",
    "        # Matmul edge_features and node_embeddings tensors\n",
    "        hidden1 = self.gcn_layer1(tf.linalg.matmul(tf.expand_dims(edge_features, axis=-1), node_embeddings)) # (batch_size, num_edge_features, num_edges, hidden_dim)\n",
    "        hidden1 = tf.reshape(hidden1, [-1, self.num_edge_features, self.hidden_dim]) # (batch_size, num_edge_features, hidden_dim)\n",
    "        hidden2 = self.gcn_layer2(tf.linalg.matmul(tf.expand_dims(edge_features, axis=-1), hidden1)) # (batch_size, num_edge_features, num_edges, hidden_dim)\n",
    "        hidden2 = tf.reshape(hidden2, [-1, self.num_edge_features, self.hidden_dim]) # (batch_size, num_edge_features, hidden_dim)\n",
    "\n",
    "        # Concatenate node features and hidden layers\n",
    "        concat_features = tf.concat([node_embeddings, hidden1, hidden2], axis=-1) # (batch_size, 1, 3*hidden_dim)\n",
    "\n",
    "        # Final prediction layer\n",
    "        predictions = self.prediction_layer(tf.squeeze(concat_features, axis=1)) # (batch_size, num_items)\n",
    "        # print(f'Predictions shape: {predictions.shape}')\n",
    "        print(f'Predictions: {predictions}')\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "class DQNCritic(tf.keras.Model):\n",
    "    def __init__(self, num_items, hidden_dim):\n",
    "        super(DQNCritic, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define dense layers\n",
    "        self.dense1 = tf.keras.layers.Dense(units=hidden_dim, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1, activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass input through dense layers\n",
    "        print(f'DQN inputs.shape {inputs.shape}')\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        # Reshape output to (batch_size, num_items)\n",
    "        q_values = tf.reshape(x, shape=(-1, self.num_items))\n",
    "        print(f'q_values {q_values.shape}')\n",
    "        return q_values\n",
    "    \n",
    "class RecommenderEnv(gym.Env):\n",
    "    def __init__(self, session_ids, item_ids, next_item_ids, next_session_ids, adj_matrix):\n",
    "        super(RecommenderEnv, self).__init__()\n",
    "\n",
    "        self.session_ids = session_ids\n",
    "        self.item_ids = item_ids\n",
    "        self.next_item_ids = next_item_ids\n",
    "        self.next_session_ids = next_session_ids\n",
    "        self.adj_matrix = adj_matrix\n",
    "\n",
    "        self.num_items = adj_matrix.shape[0]\n",
    "        self.num_sessions = len(np.unique(session_ids))\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.num_items)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.num_items,))\n",
    "\n",
    "        self.current_session = 0\n",
    "        self.current_session_items = set(self.item_ids[self.session_ids == self.current_session])\n",
    "        self.current_state = np.zeros(self.num_items)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_session = 0\n",
    "        self.current_session_items = set(self.item_ids[self.session_ids == self.current_session])\n",
    "        self.current_state = np.zeros(self.num_items)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update current state\n",
    "        self.current_state[action] = 1\n",
    "\n",
    "        # Get the reward for the action\n",
    "        if action.ref() in self.current_session_items:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        # Move to the next session if the current session has ended\n",
    "        if self.next_session_ids[self.current_session] != self.current_session:\n",
    "            self.current_session = self.next_session_ids[self.current_session]\n",
    "            self.current_session_items = set(self.item_ids[self.session_ids == self.current_session])\n",
    "\n",
    "        # Check if the episode is over\n",
    "        done = self.current_session == self.num_sessions - 1\n",
    "\n",
    "        return self.current_state, reward, done, {}\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the GNNActor model\n",
    "actor_model = GNNActor(num_items, num_features=1, num_edge_features=1, hidden_dim=32)\n",
    "\n",
    "\n",
    "# Define the DQNCritic model\n",
    "critic_model = DQNCritic(num_items, hidden_dim)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Define loss functions\n",
    "actor_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "critic_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define metrics\n",
    "actor_metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "critic_metrics = tf.keras.metrics.MeanSquaredError()\n",
    "\n",
    "# Train the models\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs}')\n",
    "    num_batches = len(session_ids) // batch_size\n",
    "    for batch_idx in range(num_batches):\n",
    "        # Select batch data\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx + 1) * batch_size\n",
    "        batch_session_ids = session_ids[start_idx:end_idx]\n",
    "        batch_item_ids = item_ids[start_idx:end_idx]\n",
    "        batch_next_item_ids = next_item_ids[start_idx:end_idx]\n",
    "        batch_adj_matrix = adj_matrix\n",
    "        \n",
    "        # Compute target Q values using critic model\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predict Q values for the next state\n",
    "            q_values_next = critic_model(batch_next_item_ids, actor_model(batch_adj_matrix(batch_item_ids)))\n",
    "            \n",
    "            # Compute target Q values\n",
    "            rewards = tf.ones_like(q_values_next)\n",
    "            q_values_target = rewards + 0.99 * tf.reduce_max(q_values_next, axis=-1)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = critic_loss_fn(q_values_target, critic_model(batch_item_ids, actor_model(batch_adj_matrix(batch_item_ids))))\n",
    "        \n",
    "        # Compute gradients and update critic model\n",
    "        critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
    "        \n",
    "        # Update critic metrics\n",
    "        critic_metrics.update_state(q_values_target, critic_model(batch_item_ids, actor_model(batch_adj_matrix(batch_item_ids))))\n",
    "        \n",
    "        # Compute action probabilities using actor model\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probabilities = actor_model(batch_adj_matrix(batch_item_ids))\n",
    "            \n",
    "        # Compute actor loss\n",
    "        actor_loss = actor_loss_fn(batch_next_item_ids, action_probabilities)\n",
    "        \n",
    "        # Compute gradients and update actor model\n",
    "        actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
    "        \n",
    "        # Update actor metrics\n",
    "        actor_metrics.update_state(batch_next_item_ids, action_probabilities)\n",
    "        \n",
    "        # Print batch metrics\n",
    "        print(f'Batch {batch_idx}/{num_batches} - Critic loss: {critic_loss}, Critic MSE: {critic_metrics.result()}, Actor loss: {actor_loss}, Actor accuracy: {actor_metrics.result()}')\n",
    "        \n",
    "    # Reset metrics\n",
    "    actor_metrics.reset_states()\n",
    "    critic_metrics.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd958a5b-5969-46ba-b40a-6cc793be2415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
