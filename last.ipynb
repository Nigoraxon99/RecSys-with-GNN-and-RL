{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185a7a0-451e-44ac-bf49-0dae1b5e18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd760f8-5584-4921-81af-9c30c186917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 10000 rows\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file paths\n",
    "data_path = 'yoochoose_dataset/yoochoose-clicks.dat'\n",
    "output_path = 'yoochoose_dataset/filtered_clicks.dat'\n",
    "\n",
    "# Open the input and output files\n",
    "with open(data_path, 'r') as f_in, open(output_path, 'w', newline='') as f_out:\n",
    "    reader = csv.reader(f_in, delimiter=',')\n",
    "    writer = csv.writer(f_out, delimiter=',')\n",
    "    \n",
    "    session_dict = {}\n",
    "    count = 0\n",
    "    \n",
    "    # Loop through the rows in the input file\n",
    "    for row in reader:\n",
    "        # Extract the session_id and item_id\n",
    "        session_id = row[0]\n",
    "        item_id = row[2]\n",
    "        \n",
    "        # Check if the session_id already exists in the dictionary\n",
    "        if session_id in session_dict:\n",
    "            # If it exists, append the item_id to the existing list\n",
    "            session_dict[session_id].append(item_id)\n",
    "        else:\n",
    "            # If it doesn't exist, create a new list with the current item_id\n",
    "            session_dict[session_id] = [item_id]\n",
    "        \n",
    "        # Check if the session length is at least 2\n",
    "        if len(session_dict[session_id]) >= 2:\n",
    "            # If it is, write the row to the output file\n",
    "            writer.writerow(row)\n",
    "            count += 1\n",
    "        \n",
    "        # Check if we've written 10000 rows to the output file\n",
    "        if count == 10000:\n",
    "            break\n",
    "    print(f\"Filtered {count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a12cfd-9c5a-4c8c-9da4-c17c2e1e3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clickstream file\n",
    "data = pd.read_csv('yoochoose_dataset/filtered_clicks.dat',\n",
    "                   names=['session_id', 'timestamp', 'item_id', 'category'],\n",
    "                   dtype={'session_id': 'int64', 'timestamp': 'str', 'item_id': 'int64', 'category': 'int64'},\n",
    "                   parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0ab38a-f7d5-42aa-b787-ca21cb936b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item and session maps\n",
    "item_map = dict(zip(np.unique(data.item_id), range(len(np.unique(data.item_id)))))\n",
    "session_map = dict(zip(np.unique(data.session_id), range(len(np.unique(data.session_id)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acfc7371-a9e0-402f-acd1-ff738c169dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map item and session IDs\n",
    "data['item_id'] = data['item_id'].map(item_map)\n",
    "data['session_id'] = data['session_id'].map(session_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d181c1a9-2409-49c3-9f49-67cd57615799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by session and timestamp\n",
    "data = data.sort_values(['session_id', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de436e9c-2848-465b-be21-d1a1f1087e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create next item and session columns\n",
    "data['next_item_id'] = data.groupby('session_id')['item_id'].shift(-1)\n",
    "data['next_session_id'] = data.groupby('session_id')['session_id'].shift(-1)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e403a74-3419-41fb-aee3-eccf12705ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "session_ids = data['session_id'].values\n",
    "item_ids = data['item_id'].values\n",
    "next_item_ids = data['next_item_id'].values\n",
    "next_session_ids = data['next_session_id'].values\n",
    "timestamps = data['timestamp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006ff155-7981-4815-b9dd-87813f602bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eac03f7-7f3c-471c-8487-9cc6ef7339c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges between items that co-occur in the same session\n",
    "for session_id in np.unique(session_ids):\n",
    "    items_in_session = item_ids[session_ids == session_id]\n",
    "    for i in range(len(items_in_session)):\n",
    "        for j in range(i + 1, len(items_in_session)):\n",
    "            if not graph.has_edge(items_in_session[i], items_in_session[j]):\n",
    "                graph.add_edge(items_in_session[i], items_in_session[j], weight=0)\n",
    "            graph[items_in_session[i]][items_in_session[j]]['weight'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0bedd3-7187-489d-a7d3-f17892deb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize edge weights\n",
    "for u, v, d in graph.edges(data=True):\n",
    "    d['weight'] /= np.sqrt(graph.degree(u) * graph.degree(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f13c7e7-8735-491b-999d-ba3ba3d211cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2273, 2273)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = sp.coo_matrix(nx.to_numpy_array(graph, weight='weight', dtype=np.float32))\n",
    "adj_matrix = tf.sparse.SparseTensor(indices=np.array([adj_matrix.row, adj_matrix.col]).T,\n",
    "                                    values=adj_matrix.data,\n",
    "                                    dense_shape=adj_matrix.shape)\n",
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8104feb6-e4c4-4cfb-b8fb-56e78f6435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_nodes = adj_matrix.shape[0]\n",
    "embedding_dim = 32\n",
    "num_layers = 2\n",
    "temperature = 0.07\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "num_classes = len(np.unique(item_ids))\n",
    "node_ids = tf.keras.Input(shape=(num_nodes,), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da9fa2fe-1622-4e5d-a65d-0f9f2d7ddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_hidden=16, num_layers=2, num_classes=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # define dense layers\n",
    "        self.dense_layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(num_hidden, activation=\"relu\"))\n",
    "            \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=num_nodes, output_dim=num_hidden)\n",
    "        \n",
    "        # define final classification layer\n",
    "        self.classification_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        node_ids, adj_matrix = inputs\n",
    "        \n",
    "        # create node embeddings\n",
    "        x = self.embedding(node_ids)\n",
    "        \n",
    "        # apply dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            # Transpose the feature matrix before multiplying with the adjacency matrix\n",
    "            x = layer(tf.transpose(x, perm=[0, 2, 1]))\n",
    "            x = tf.transpose(x, perm=[0, 2, 1])\n",
    "            \n",
    "            # apply dropout\n",
    "            x = tf.keras.layers.Dropout(0.5)(x, training=kwargs.get(\"training\", False))\n",
    "            \n",
    "            # apply skip connection\n",
    "            x = x + self.embedding(node_ids)\n",
    "            \n",
    "            # apply normalization\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            # apply activation\n",
    "            x = tf.keras.activations.relu(x)\n",
    "            \n",
    "            # multiply with adjacency matrix\n",
    "            x = tf.sparse.sparse_dense_matmul(adj_matrix, x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e91c0773-7bf5-46ed-9f48-2e0f5eb03ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contrastive loss function\n",
    "def contrastive_loss(y_true, y_pred, temperature):\n",
    "    logits = tf.matmul(y_pred, tf.transpose(y_pred)) / temperature\n",
    "    labels = tf.one_hot(tf.range(tf.shape(y_pred)[0]), tf.shape(y_pred)[0] * 2)\n",
    "    mask = 1 - tf.eye(tf.shape(y_pred)[0], dtype=tf.int32)\n",
    "    labels = tf.reshape(labels, (-1, tf.shape(y_pred)[0] * 2))\n",
    "    mask = tf.reshape(mask, (-1,))\n",
    "    labels = tf.boolean_mask(labels, mask)\n",
    "    logits = tf.boolean_mask(logits, mask)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85ba5509-f63b-48d3-be27-0376363fd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GNN model\n",
    "gnn = GNN(embedding_dim, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3338351-2340-46ed-9110-5de8afd2efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e948957-bfaa-4d2f-b7ae-65703eeaeb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2273, 2273)\n"
     ]
    }
   ],
   "source": [
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e779ec5-c726-4dd9-adc1-272cd11b5164",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'gnn_2' (type GNN).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]\n\nCall arguments received by layer 'gnn_2' (type GNN):\n  • inputs=['tf.Tensor(shape=(128,), dtype=int64)', 'SparseTensor(indices=tf.Tensor(\\n[[  0   1]\\n [  1   0]\\n [  2   3]\\n ...\\n [127 135]\\n [127 136]\\n [127 137]], shape=(3365, 2), dtype=int64), values=tf.Tensor([1.         1.         0.07332356 ... 0.15430336 0.15430336 0.15430336], shape=(3365,), dtype=float32), dense_shape=tf.Tensor([ 128 2273], shape=(2,), dtype=int64))']\n  • kwargs={'training': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_adj_matrix \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mslice(adj_matrix, [i, \u001b[38;5;241m0\u001b[39m], [batch_size, adj_matrix\u001b[38;5;241m.\u001b[39mdense_shape[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# mask = tf.equal(batch_node_ids[:, tf.newaxis], adj_matrix.indices[:, 0])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# mask = tf.transpose(mask)  # transpose to match the expected shape\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# indices = tf.boolean_mask(adj_matrix.indices, mask)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     dense_shape=[len(batch_node_ids), adj_matrix.dense_shape[1]]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_node_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_adj_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([batch_features, batch_features], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m contrastive_loss(\u001b[38;5;28;01mNone\u001b[39;00m, batch_labels, temperature)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m, in \u001b[0;36mGNN.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# apply dense layers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_layers:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Transpose the feature matrix before multiplying with the adjacency matrix\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(x, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# apply dropout\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'gnn_2' (type GNN).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]\n\nCall arguments received by layer 'gnn_2' (type GNN):\n  • inputs=['tf.Tensor(shape=(128,), dtype=int64)', 'SparseTensor(indices=tf.Tensor(\\n[[  0   1]\\n [  1   0]\\n [  2   3]\\n ...\\n [127 135]\\n [127 136]\\n [127 137]], shape=(3365, 2), dtype=int64), values=tf.Tensor([1.         1.         0.07332356 ... 0.15430336 0.15430336 0.15430336], shape=(3365,), dtype=float32), dense_shape=tf.Tensor([ 128 2273], shape=(2,), dtype=int64))']\n  • kwargs={'training': 'None'}"
     ]
    }
   ],
   "source": [
    "# Train GNN model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(session_ids), batch_size):\n",
    "        batch_node_ids = session_ids[i:i + batch_size]\n",
    "        # Extract the submatrix corresponding to the batch\n",
    "        batch_adj_matrix = tf.sparse.slice(adj_matrix, [i, 0], [batch_size, adj_matrix.dense_shape[1]])\n",
    "\n",
    "        # mask = tf.equal(batch_node_ids[:, tf.newaxis], adj_matrix.indices[:, 0])\n",
    "        # mask = tf.transpose(mask)  # transpose to match the expected shape\n",
    "        # indices = tf.boolean_mask(adj_matrix.indices, mask)\n",
    "        # values = tf.boolean_mask(adj_matrix.values, mask)\n",
    "        # batch_adj_matrix = tf.sparse.SparseTensor(\n",
    "        #     indices=indices,\n",
    "        #     values=values,\n",
    "        #     dense_shape=[len(batch_node_ids), adj_matrix.dense_shape[1]]\n",
    "        # )\n",
    "        batch_features = gnn([batch_node_ids, batch_adj_matrix])\n",
    "        batch_labels = tf.concat([batch_features, batch_features], axis=0)\n",
    "        batch_loss = contrastive_loss(None, batch_labels, temperature)\n",
    "        grads = tf.gradients(batch_loss, gnn.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, gnn.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c348126-0d66-4de5-af46-502c281c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.        1.        0.        ... 0.        0.        0.       ]\n",
      " [1.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " ...\n",
      " [0.        0.        0.        ... 0.2       0.5163978 0.       ]\n",
      " [0.        0.        0.        ... 0.5163978 0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.25     ]], shape=(2273, 2273), dtype=float32)\n",
      "SparseTensor(indices=tf.Tensor(\n",
      "[[   0    1]\n",
      " [   1    0]\n",
      " [   2    3]\n",
      " ...\n",
      " [2272 1478]\n",
      " [2272 1479]\n",
      " [2272 2272]], shape=(30744, 2), dtype=int64), values=tf.Tensor([1.         1.         0.07332356 ... 0.70710677 0.70710677 0.25      ], shape=(30744,), dtype=float32), dense_shape=tf.Tensor([2273 2273], shape=(2,), dtype=int64))\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'gnn_1' (type GNN).\n\n{{function_node __wrapped__SparseTensorDenseMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot multiply A and B because inner dimension does not match: 2273 vs. 64.  Did you forget a transpose?  Dimensions of A: [2132, 2273).  Dimensions of B: [64,32] [Op:SparseTensorDenseMatMul]\n\nCall arguments received by layer 'gnn_1' (type GNN):\n  • inputs=['tf.Tensor(shape=(64,), dtype=int64)', 'SparseTensor(indices=tf.Tensor(\\n[[   0   85]\\n [   0   86]\\n [   0   87]\\n ...\\n [2131 2236]\\n [2131 2237]\\n [2131 2238]], shape=(27910, 2), dtype=int64), values=tf.Tensor([0.14285715 0.14285715 0.14285715 ... 0.03571429 0.03571429 0.03571429], shape=(27910,), dtype=float32), dense_shape=tf.Tensor([2132 2273], shape=(2,), dtype=int64))']\n  • kwargs={'training': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node_ids, batch_adj_matrix\n\u001b[1;32m     29\u001b[0m batch_node_ids, batch_adj_matrix \u001b[38;5;241m=\u001b[39m get_batch(node_ids, adj_matrix)\n\u001b[0;32m---> 30\u001b[0m batch_emb \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_node_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_adj_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([batch_emb, batch_emb], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m train_step(batch_labels)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mGNN.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(node_ids)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_layers:\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_dense_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'gnn_1' (type GNN).\n\n{{function_node __wrapped__SparseTensorDenseMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot multiply A and B because inner dimension does not match: 2273 vs. 64.  Did you forget a transpose?  Dimensions of A: [2132, 2273).  Dimensions of B: [64,32] [Op:SparseTensorDenseMatMul]\n\nCall arguments received by layer 'gnn_1' (type GNN):\n  • inputs=['tf.Tensor(shape=(64,), dtype=int64)', 'SparseTensor(indices=tf.Tensor(\\n[[   0   85]\\n [   0   86]\\n [   0   87]\\n ...\\n [2131 2236]\\n [2131 2237]\\n [2131 2238]], shape=(27910, 2), dtype=int64), values=tf.Tensor([0.14285715 0.14285715 0.14285715 ... 0.03571429 0.03571429 0.03571429], shape=(27910,), dtype=float32), dense_shape=tf.Tensor([2132 2273], shape=(2,), dtype=int64))']\n  • kwargs={'training': 'None'}"
     ]
    }
   ],
   "source": [
    "# Train GNN model\n",
    "for epoch in range(num_epochs):\n",
    "    batch_size = 64\n",
    "    num_batches = int(np.ceil(len(session_ids) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # get batch node ids\n",
    "        batch_node_ids = session_ids[i:i + batch_size]\n",
    "\n",
    "        # assuming adj_matrix is a sparse tensor\n",
    "        dense_adj_matrix = tf.sparse.to_dense(adj_matrix)\n",
    "        dense_adj_matrix = tf.convert_to_tensor(dense_adj_matrix, dtype=tf.float32)\n",
    "        print(dense_adj_matrix)\n",
    "        print(adj_matrix)\n",
    "\n",
    "        indices = tf.where(tf.not_equal(dense_adj_matrix, 0))\n",
    "        values = tf.gather_nd(dense_adj_matrix, indices)\n",
    "\n",
    "        # construct sparse tensor from indices, values and dense shape\n",
    "        adj = tf.SparseTensor(indices, values, dense_shape=dense_adj_matrix.shape)\n",
    "        node_ids = np.random.choice(adj_matrix.shape[0], size=batch_size, replace=False)\n",
    "        \n",
    "        def get_batch(node_ids, adj_matrix):\n",
    "            node_ids = np.random.choice(adj_matrix.shape[0], size=batch_size, replace=False)\n",
    "            min_id = tf.reduce_min(node_ids)\n",
    "            max_id = tf.reduce_max(node_ids)\n",
    "            batch_adj_matrix = tf.sparse.slice(adj_matrix, start=[min_id, 0], size=[max_id - min_id + 1, adj_matrix.shape[1]])\n",
    "            return node_ids, batch_adj_matrix\n",
    "        batch_node_ids, batch_adj_matrix = get_batch(node_ids, adj_matrix)\n",
    "        batch_emb = gnn([batch_node_ids, batch_adj_matrix], training=None).numpy()\n",
    "        batch_labels = tf.concat([batch_emb, batch_emb], axis=0)\n",
    "        train_step(batch_labels)\n",
    "        batch_loss = contrastive_loss(None, batch_labels, temperature)\n",
    "        grads = tf.gradients(batch_loss, gnn.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, gnn.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa289dd2-97e9-45db-98b1-ab92d705cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define session-based recommender system with reinforcement learning\n",
    "class RecommenderSystem:\n",
    "    def __init__(self, gnn, item_map, gamma=0.9, alpha=0.1):\n",
    "        self.gnn = gnn\n",
    "        self.item_map = item_map\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.replay_buffer = deque(maxlen=10000)\n",
    "        self.session_history = []\n",
    "\n",
    "    def recommend(self, session_items):\n",
    "        session_node_ids = [self.item_map[item] for item in session_items if item in self.item_map]\n",
    "        if len(session_node_ids) == 0:\n",
    "            return []\n",
    "        session_adj_matrix = tf.sparse.SparseTensor(indices=adj_matrix.indices,\n",
    "                                                    values=adj_matrix.values[np.isin(adj_matrix.row, session_node_ids)],\n",
    "                                                    dense_shape=adj_matrix.dense_shape)\n",
    "        session_features = gnn([session_node_ids, session_adj_matrix])\n",
    "        item_scores = np.matmul(session_features, gnn.embedding.weights[0].numpy().T)\n",
    "        item_scores[np.isin(np.arange(len(item_map)), session_node_ids)] = -np.inf\n",
    "        item_indices = np.argsort(item_scores)[::-1]\n",
    "        return [item_map[i] for i in item_indices[:10]]\n",
    "\n",
    "    def update(self, session_items, reward):\n",
    "        session_node_ids = [self.item_map[item] for item in session_items if item in self.item_map]\n",
    "        if len(session_node_ids) == 0:\n",
    "            return\n",
    "        session_adj_matrix = tf.sparse.SparseTensor(indices=adj_matrix.indices,\n",
    "                                                    values=adj_matrix.values[np.isin(adj_matrix.row, session_node_ids)],\n",
    "                                                    dense_shape=adj_matrix.dense_shape)\n",
    "        session_features = gnn([session_node_ids, session_adj_matrix])\n",
    "        item_scores = np.matmul(session_features, gnn.embedding.weights[0].numpy().T)\n",
    "        item_indices = np.argsort(item_scores)[::-1]\n",
    "        item_probs = np.exp(item_scores) / np.sum(np.exp(item_scores))\n",
    "        item_probs[np.isin(np.arange(len(item_map)), session_node_ids)] = 0\n",
    "        item_probs = item_probs / np.sum(item_probs)\n",
    "        item_rewards = np.zeros(len(item_map))\n",
    "        item_rewards[item_indices[:10]] = reward\n",
    "        self.replay_buffer.append((session_features.numpy(), item_probs, item_rewards))\n",
    "        self.session_history.append(session_node_ids)\n",
    "\n",
    "        if len(self.replay_buffer) == self.replay_buffer.maxlen:\n",
    "            for i in range(self.replay_buffer.maxlen):\n",
    "                session_features, item_probs, item_rewards = self.replay_buffer[i]\n",
    "                discounted_rewards = np.zeros(len(item_map))\n",
    "                running_reward = 0\n",
    "                for j in range(len(item_map)):\n",
    "                    if item_rewards[j] != 0:\n",
    "                        running_reward = item_rewards[j]\n",
    "                    else:\n",
    "                        running_reward = running_reward * self.gamma\n",
    "                    discounted_rewards[j] = running_reward\n",
    "                item_values = np.sum(np.exp(np.matmul(session_features, gnn.embedding.weights[0].numpy().T)) * discounted_rewards,axis=1)\n",
    "                item_grads = tf.gradients(tf.math.log(item_probs), gnn.trainable_weights, grad_ys=item_values)\n",
    "                optimizer.apply_gradients(zip(item_grads, gnn.trainable_weights))\n",
    "\n",
    "            self.replay_buffer.clear()\n",
    "            self.session_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc15f9b1-a40e-4213-852e-08a7be855960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "MSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the GNN using precision, recall, and mean squared error\n",
    "buy_events = pd.read_csv('yoochoose_dataset/yoochoose-buys.dat', header=None, usecols=[0, 2, 3], names=['session_id', 'item_id', 'Price'])\n",
    "buy_events = buy_events[buy_events['item_id'].isin(item_map.keys())].groupby('session_id')['item_id'].apply(set).reset_index()\n",
    "buy_events['item_id'] = buy_events['item_id'].apply(lambda x: list(x))\n",
    "\n",
    "test_clickstream = data.sample(frac=0.2)\n",
    "test_clickstream['item_id'] = test_clickstream['item_id'].apply(lambda x: item_map.get(x, -1))\n",
    "test_clickstream = test_clickstream[test_clickstream['item_id'] != -1]\n",
    "\n",
    "# test_sessions = test_clickstream.groupby('session_id')['item_id'].apply(list).reset_index()\n",
    "test_sessions = test_clickstream.groupby('session_id', group_keys=False)['item_id'].apply(list).reset_index()\n",
    "\n",
    "\n",
    "test_session_graphs = []\n",
    "for session_items in test_sessions['item_id']:\n",
    "    session_nodes = set(session_items)\n",
    "    session_adj_matrix = adj_matrix.copy()\n",
    "    session_adj_matrix = tf.sparse.SparseTensor(indices=session_adj_matrix.indices,\n",
    "                                                values=session_adj_matrix.values[\n",
    "                                                    np.isin(session_adj_matrix.row, session_nodes)],\n",
    "                                                dense_shape=session_adj_matrix.dense_shape)\n",
    "    session_features = gnn([session_nodes, session_adj_matrix])\n",
    "    test_session_graphs.append(session_features.numpy())\n",
    "print(test_session_graphs)\n",
    "precision = 0\n",
    "recall = 0\n",
    "mse = 0\n",
    "item_map_inv = {v: k for k, v in item_map.items()}\n",
    "for i, session_graph in enumerate(test_session_graphs):\n",
    "    session_items = test_sessions['item_id'][i]\n",
    "    item_scores = np.matmul(session_graph, gnn.embedding.weights[0].numpy().T)\n",
    "    item_indices = np.argsort(item_scores)[::-1]\n",
    "    recommended_items = item_indices[:10]\n",
    "    recommended_items = [item_map_inv[x] for x in recommended_items if x in item_map_inv]\n",
    "    purchased_items = buy_events[buy_events['session_id'] == test_sessions['session_id'][i]]['item_id'].iloc[0]\n",
    "    true_positives = set(recommended_items).intersection(purchased_items)\n",
    "    precision += len(true_positives) / len(recommended_items)\n",
    "    recall += len(true_positives) / len(purchased_items)\n",
    "    mse += mean_squared_error(item_scores, [1 if x in purchased_items else 0 for x in range(len(item_map))]).numpy()\n",
    "if len(test_session_graphs) > 0:\n",
    "    precision /= len(test_session_graphs)\n",
    "    recall /= len(test_session_graphs)\n",
    "    mse /= len(test_session_graphs)\n",
    "else:\n",
    "    precision, recall, mse = 0.0, 0.0, 0.0\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('MSE:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da29e3-275c-478a-adb2-1f246446f645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
