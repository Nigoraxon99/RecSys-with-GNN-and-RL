{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a381420e-46b7-4776-9b61-2bdae1b6ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3007, 3007)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, Embedding\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy as sp\n",
    "\n",
    "# Data preprocessing\n",
    "# Load the Yoochoose dataset and preprocess it to create session-based sequences of interactions\n",
    "data = pd.read_csv('yoochoose_dataset/filtered_clicks.dat',\n",
    "                   names=['session_id', 'timestamp', 'item_id', 'category'],\n",
    "                   dtype={'session_id': 'int64', 'timestamp': 'str', 'item_id': 'int64', 'category': 'int64'},\n",
    "                   parse_dates=['timestamp'])\n",
    "\n",
    "# Create item and session maps\n",
    "item_map = dict(zip(np.unique(data.item_id), range(len(np.unique(data.item_id)))))\n",
    "session_map = dict(zip(np.unique(data.session_id), range(len(np.unique(data.session_id)))))\n",
    "\n",
    "# Map item and session IDs\n",
    "data['item_id'] = data['item_id'].map(item_map)\n",
    "data['session_id'] = data['session_id'].map(session_map)\n",
    "\n",
    "# Sort by session and timestamp\n",
    "data = data.sort_values(['session_id', 'timestamp'])\n",
    "\n",
    "# Create next item and session columns\n",
    "data['next_item_id'] = data.groupby('session_id')['item_id'].shift(-1)\n",
    "data['next_session_id'] = data.groupby('session_id')['session_id'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "session_ids = data['session_id'].values.astype('int32')\n",
    "item_ids = data['item_id'].values.astype('int32')\n",
    "next_item_ids = data['next_item_id'].values.astype('int32')\n",
    "next_session_ids = data['next_session_id'].values.astype('int32')\n",
    "timestamps = data['timestamp'].values\n",
    "\n",
    "# Create a directed graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(item_map.values())\n",
    "\n",
    "# Add edges between items that co-occur in the same session\n",
    "for session_id in np.unique(session_ids):\n",
    "    items_in_session = item_ids[session_ids == session_id]\n",
    "    for i in range(len(items_in_session)):\n",
    "        for j in range(i + 1, len(items_in_session)):\n",
    "            if not graph.has_edge(items_in_session[i], items_in_session[j]):\n",
    "                graph.add_edge(items_in_session[i], items_in_session[j], weight=0)\n",
    "            graph[items_in_session[i]][items_in_session[j]]['weight'] += 1\n",
    "\n",
    "# Normalize edge weights\n",
    "for u, v, d in graph.edges(data=True):\n",
    "    d['weight'] /= np.sqrt(graph.degree(u) * graph.degree(v))            \n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = coo_matrix(nx.to_numpy_array(graph, weight='weight', dtype=np.float32))\n",
    "adj_matrix = tf.sparse.SparseTensor(indices=np.array([adj_matrix.row, adj_matrix.col]).T,\n",
    "                                    values=adj_matrix.data,\n",
    "                                    dense_shape=adj_matrix.shape)    \n",
    "print(adj_matrix.shape)\n",
    "num_nodes = adj_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39ab80fc-25fb-4366-902c-14e062d949d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_hidden=16, num_layers=2, num_classes=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # define dense layers\n",
    "        self.dense_layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(num_hidden, activation=\"relu\"))\n",
    "            \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=num_nodes, output_dim=num_hidden)\n",
    "        \n",
    "        # define final classification layer\n",
    "        self.classification_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        node_ids, adj_matrix = inputs\n",
    "        \n",
    "        # create node embeddings\n",
    "        x = self.embedding(node_ids)\n",
    "        \n",
    "        # apply dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            # Transpose the feature matrix before multiplying with the adjacency matrix\n",
    "            x = layer(tf.transpose(x, perm=[0, 2, 1]))\n",
    "            x = tf.transpose(x, perm=[0, 2, 1])\n",
    "            \n",
    "            # apply dropout\n",
    "            x = tf.keras.layers.Dropout(0.5)(x, training=kwargs.get(\"training\", False))\n",
    "            \n",
    "            # apply skip connection\n",
    "            x = x + self.embedding(node_ids)\n",
    "            \n",
    "            # apply normalization\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            # apply activation\n",
    "            x = tf.keras.activations.relu(x)\n",
    "            \n",
    "            # multiply with adjacency matrix\n",
    "            x = tf.sparse.sparse_dense_matmul(adj_matrix, x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b5a33-684b-4a60-87f3-61b03099fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actor model\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, num_actions, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_size, activation='relu')\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(num_actions, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embeddings = inputs\n",
    "        x = self.dense1(embeddings)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab6173-6e80-4b2b-898b-94d23d6af174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN model\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, num_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_size, activation='relu')\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(num_actions)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        embeddings, action = inputs\n",
    "        x = tf.concat([embeddings, action], axis=-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66453362-0fca-42fc-832d-72b1d67f1949",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'gnn_12' (type GNN).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]\n\nCall arguments received by layer 'gnn_12' (type GNN):\n  • inputs=['tf.Tensor(shape=(64,), dtype=int32)', 'tf.Tensor(shape=(114, 114), dtype=float32)']\n  • kwargs={'training': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# graph = nx.from_pandas_edgelist(batch_data, source='item_id', target='next_item_id', create_using=nx.DiGraph())\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# graph = nx.to_numpy_array(graph, dtype=np.float32)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# graph = tf.convert_to_tensor(graph)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 28\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(graph, embeddings)\n\u001b[1;32m     30\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, gnn\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-science/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[61], line 28\u001b[0m, in \u001b[0;36mGNN.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# apply dense layers\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_layers:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Transpose the feature matrix before multiplying with the adjacency matrix\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(x, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# apply dropout\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'gnn_12' (type GNN).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 2. But input(1) is a vector of size 3 [Op:Transpose]\n\nCall arguments received by layer 'gnn_12' (type GNN):\n  • inputs=['tf.Tensor(shape=(64,), dtype=int32)', 'tf.Tensor(shape=(114, 114), dtype=float32)']\n  • kwargs={'training': 'None'}"
     ]
    }
   ],
   "source": [
    "#define hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 32\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# split data into train and validation\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "# Initialize GNN model\n",
    "gnn = GNN(num_items, embedding_dim)\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_epochs):\n",
    "    for j in range(0, len(train_data), batch_size):\n",
    "        batch_data = train_data[j:j+batch_size]\n",
    "        item_ids = batch_data['item_id'].values.astype('int32')\n",
    "        # graph = nx.from_pandas_edgelist(batch_data, source='item_id', target='next_item_id', create_using=nx.DiGraph())\n",
    "        # graph = nx.to_numpy_array(graph, dtype=np.float32)\n",
    "        # graph = tf.convert_to_tensor(graph)\n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings = gnn([item_ids, graph])\n",
    "            loss = loss_fn(graph, embeddings)\n",
    "        gradients = tape.gradient(loss, gnn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gnn.trainable_variables))\n",
    "for j in range(0, len(val_data), batch_size):\n",
    "    batch_data = val_data[j:j+batch_size]\n",
    "    item_ids = batch_data['item_id'].values.astype('int32')\n",
    "    graph = nx.from_pandas_edgelist(batch_data, source='item_id', target='next_item_id', create_using=nx.DiGraph())\n",
    "    graph = nx.to_numpy_array(graph, dtype=np.float32)\n",
    "    graph = tf.convert_to_tensor(graph)\n",
    "    embeddings = gnn([item_ids, graph])\n",
    "    val_loss = loss_fn(graph, embeddings)\n",
    "    total_val_loss += val_loss\n",
    "val_loss = total_val_loss / len(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3206812f-4d81-49c2-8e51-75c5045d565d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276.25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.5*-1.25)+(-2.5*-6.25)+(6.5*13.75)+(1.5*5.75)+(-12.5*-12.25)+(4.5*1.75)+(-1.5*-3.25)+(0.5*-0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74486d4e-3fe9-4394-8af6-dfd7c21a4706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.464285714285715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "276.25/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986fabc9-4f3d-4fe6-82b3-4a1327b937bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.575000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.5*0.725)+(-2.5*-0.375)+(6.5*-0.575)+(1.5*-0.475)+(-12.5*-0.275)+(4.5*0.225)+(-1.5*0.325)+(0.5*0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af3077f-44ed-4372-9c5c-2e830b03ac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.367857142857143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.575000000000001/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427d044a-f351-44f3-ad8e-236786c6e482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.6499999999999995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.25*0.725)+(-6.25*-0.375)+(13.75*-0.575)+(5.75*-0.475)+(-12.25*-0.275)+(1.75*0.225)+(-3.25*0.325)+(-0.25*0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56712ea3-4007-4157-89bb-5b7c927fe747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-6.6499999999999995/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bbbee6-638a-4cca-9bc9-0935f693ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5*2.5*2 + 6.5*6.5 + 1.5*1.5 + 12.5*12.5 + 4.5*4.5 + 1.5*1.5 + 0.5*0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09b8e47-ffec-49e9-a52b-065beb279eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.714285714285715"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "236/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd28129e-fa96-4098-9e02-325b50e9c7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.25*1.25 + 6.25*6.25 + 13.75*13.75 + 5.75*5.75 +12.25*12.25 + 1.75*1.75 + 3.25*3.25 + 0.25*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd25ea6c-d9b8-46b9-b16f-f172dfc83adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.92857142857143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "426.5/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91e47b56-fc1c-4182-95e8-fe0517f83dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.845"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.725*0.725 + 0.375*0.375 + 0.575*0.575 + 0.475*0.475 + 0.275*0.275 + 0.225*0.225 + 0.325*0.325 + 0.625*0.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c683631-f21e-4b65-b2ec-8f29c6a7dd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26357142857142857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.845/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65d36fc-2ff5-475a-875d-e36af8b77d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msqr\u001b[49m(\u001b[38;5;241m33.7\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqr' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e18e6c-8840-47d4-a56f-cf2f2885ec6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
